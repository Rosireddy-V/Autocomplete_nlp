{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP3NmNOBMg8Pfrir+r+Rm6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rosireddy-V/Autocomplete_nlp/blob/main/Autocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmWUX-INURku",
        "outputId": "0a3bdc87-bbcf-4f4b-bada-75ccc5d4b4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
        "corpus = corpus.lower()\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZV3uXnRoTv6",
        "outputId": "8436fc26-6f05-4e8e-f215-70933627d313"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\",corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlwvoIsuoxCY",
        "outputId": "2755fd94-20fc-4981-8a01-e86ca28996ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning makes me happy. i am happy because i am learning! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_date=\"Sat May  9 07:33:35 CEST 2020\"\n",
        "date_parts = input_date.split(\" \")\n",
        "print(f\"date parts = {date_parts}\")\n",
        "time_parts = date_parts[4].split(\":\")\n",
        "print(f\"time parts = {time_parts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RV-wYw2qbhb",
        "outputId": "e07238e3-4f3d-40db-8bad-eb321898e98b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date parts = ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
            "time parts = ['07', '33', '35']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'i am happy because i am learning.'\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "print(f'{sentence} -> {tokenized_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRYVks5kroHX",
        "outputId": "1982a4a5-6336-4257-cd47-347e0a62b2c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "word_lengths = [(word, len(word)) for word in tokenized_sentence]\n",
        "print(f' Lengths of the words: \\n{word_lengths}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k4MdO62r0s_",
        "outputId": "9cafd538-2814-4351-98bf-21d443eb81ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Lengths of the words: \n",
            "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trigrams(sentence):\n",
        "  for i in range(len(sentence)-3+1):\n",
        "    trigram=sentence[i:i+3]\n",
        "    print(trigram)\n",
        "\n",
        "trigrams(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsURz1CDsSZO",
        "outputId": "623a830b-115c-49e5-f220-38edaa79e1ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'happy']\n",
            "['am', 'happy', 'because']\n",
            "['happy', 'because', 'i']\n",
            "['because', 'i', 'am']\n",
            "['i', 'am', 'learning']\n",
            "['am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fourgram = ['i', 'am', 'happy','because']\n",
        "trigram=fourgram[0:-1]\n",
        "print(trigram)\n",
        "fourgram.index('i')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57cwCk89tLC1",
        "outputId": "a8a6332c-1ad3-431f-dc8f-12c70a02f1a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'happy']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "tokenized_sentence=[\"<s>\"]*(n-1)+tokenized_sentence+[\"<e>\"]\n",
        "tokenized_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl1rAz-dEQPk",
        "outputId": "2604c88a-6262-480f-d60e-4fbce30bf714"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '<s>',\n",
              " 'i',\n",
              " 'am',\n",
              " 'happy',\n",
              " 'because',\n",
              " 'i',\n",
              " 'am',\n",
              " 'learning',\n",
              " '.',\n",
              " '<e>']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts = {\n",
        "    ('i', 'am', 'happy'): 2,\n",
        "    ('am', 'happy', 'because'): 1}\n",
        "print(f\"count of n-gram {('i', 'am', 'happy')}: {n_gram_counts[('i', 'am', 'happy')]}\")\n",
        "if ('i', 'am', 'learning') in n_gram_counts:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
        "else:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n",
        "\n",
        "n_gram_counts[('i', 'am', 'learning')] = 1\n",
        "if ('i', 'am', 'learning') in n_gram_counts:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
        "else:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} missing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN1vb7pfExlS",
        "outputId": "382e65a3-d28a-4652-dca4-2edbdc5fa2f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count of n-gram ('i', 'am', 'happy'): 2\n",
            "n-gram ('i', 'am', 'learning') missing\n",
            "n-gram ('i', 'am', 'learning') found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = ('i', 'am', 'happy')\n",
        "word = 'because'\n",
        "n_gram = prefix + (word,)\n",
        "print(n_gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVDPc4DjMV7d",
        "outputId": "0360911f-526b-4ddf-b783-dbc89bb52783"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('i', 'am', 'happy', 'because')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "def single_pass_trigram_count(corpus):\n",
        "  bigrams=[]\n",
        "  vocabulary=[]\n",
        "  count_dictionary=defaultdict(dict)\n",
        "  for i in range(len(corpus)-3+1):\n",
        "    trigram=tuple(corpus[i:i+3])\n",
        "    bigram=trigram[0:-1]\n",
        "    if bigram not in bigrams:\n",
        "      bigrams.append(bigram)\n",
        "    last_word=trigram[-1]\n",
        "    if last_word not in vocabulary:\n",
        "      vocabulary.append(last_word)\n",
        "    if (bigram,last_word) not in count_dictionary:\n",
        "      count_dictionary[(bigram,last_word)]=0\n",
        "\n",
        "    count_dictionary[(bigram,last_word)]+=1\n",
        "\n",
        "  count_matrix=np.zeros((len(bigrams),len(vocabulary)))\n",
        "  for trigramkey,trigramcount in count_dictionary.items():\n",
        "    count_matrix[bigrams.index(trigramkey[0]),vocabulary.index(trigramkey[1])]=trigramcount\n",
        "\n",
        "  count_matrix=pd.DataFrame(count_matrix,index=bigrams,columns=vocabulary)\n",
        "\n",
        "  return bigrams,vocabulary,count_matrix\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yBCC_II9NFVU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "bigrams, vocabulary, count_matrix = single_pass_trigram_count(corpus)\n",
        "\n",
        "print(count_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhGudVxbBxrT",
        "outputId": "5bf373ed-c483-46f3-d4f4-90aece781b6a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  happy  because    i   am  learning    .\n",
            "(i, am)             1.0      0.0  0.0  0.0       1.0  0.0\n",
            "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
            "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
            "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
            "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rows_sum=count_matrix.sum(axis=1)\n",
        "print(rows_sum)\n",
        "prob_matrix=count_matrix.div(rows_sum,axis=0)\n",
        "print(prob_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1I8MZ0lCDuM",
        "outputId": "2012a8a8-63e2-41d7-c614-9288bafa50e9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(i, am)             2.0\n",
            "(am, happy)         1.0\n",
            "(happy, because)    1.0\n",
            "(because, i)        1.0\n",
            "(am, learning)      1.0\n",
            "dtype: float64\n",
            "                  happy  because    i   am  learning    .\n",
            "(i, am)             0.5      0.0  0.0  0.0       0.5  0.0\n",
            "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
            "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
            "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
            "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram=('i','am','happy')\n",
        "bigram=trigram[:-1]\n",
        "print(f\"bigram: {bigram}\")\n",
        "word=trigram[-1]\n",
        "print(f\"word: {word}\")\n",
        "trigram_prob=prob_matrix[word][bigram]\n",
        "print(\"trigram probability: \",trigram_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tWI-3z5Fc59",
        "outputId": "39ec65c9-c8d2-4002-a23b-c742bbf1fd6a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram: ('i', 'am')\n",
            "word: happy\n",
            "trigram probability:  0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = ['i', 'am', 'happy', 'because', 'learning', '.', 'have', 'you', 'seen','it', '?']\n",
        "starts_with = 'ha'\n",
        "\n",
        "print(f'words in vocabulary starting with prefix: {starts_with}\\n')\n",
        "for word in vocabulary:\n",
        "    if word.startswith(starts_with):\n",
        "        print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiPy___6VZ8P",
        "outputId": "110a4bf1-f5e5-4b8b-d7f1-5fb9454e763f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words in vocabulary starting with prefix: ha\n",
            "\n",
            "happy\n",
            "have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def train_test_validation_split(data,train_percent,validation_percent):\n",
        "  random.seed(87)\n",
        "  random.shuffle(data)\n",
        "  train_size=int((len(data)*train_percent)/100)\n",
        "  train_data=data[:train_size]\n",
        "  validation_size=int((len(data)*validation_percent)/100)\n",
        "  validation_data=data[train_size:train_size+validation_size]\n",
        "  test_data=data[train_size+validation_size:]\n",
        "\n",
        "  return train_data, validation_data, test_data\n",
        "\n",
        "data = [x for x in range (0, 100)]\n",
        "\n",
        "train_data, validation_data, test_data = train_test_validation_split(data, 80, 10)\n",
        "print(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\",\n",
        "      f\"test data:{test_data}\\n\")\n",
        "\n",
        "train_data, validation_data, test_data = train_test_validation_split(data, 98, 1)\n",
        "print(\"split 98/1/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\",\n",
        "      f\"test data:{test_data}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8mP3XSFXYNm",
        "outputId": "0bcd5708-d55b-427a-d459-5c251b0f22c4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split 80/10/10:\n",
            " train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n",
            " validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n",
            " test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n",
            "\n",
            "split 98/1/1:\n",
            " train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n",
            " validation data:[35]\n",
            " test data:[75]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = 10 ** (-250)\n",
        "M = 100\n",
        "perplexity = p ** (-1 / M)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx3zRFzwc1jX",
        "outputId": "9aadf68d-21f3-4c17-c5b3-04b1d17bdecb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "316.22776601683796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "m=3\n",
        "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n",
        "\n",
        "vocabulary = Counter(word_counts).most_common(m)\n",
        "vocabulary = [w[0] for w in vocabulary]\n",
        "\n",
        "print(f\"the new vocabulary containing {m} most frequent words: {vocabulary}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwOUM-npdP2g",
        "outputId": "212e40d3-0740-484e-cbe0-842f814eb01e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=['am','i','learning']\n",
        "output_sentence=[]\n",
        "for w in sentence:\n",
        "  if w in vocabulary:\n",
        "    output_sentence.append(w)\n",
        "  else:\n",
        "    output_sentence.append('<unk>')\n",
        "print(sentence)\n",
        "print(output_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJpfux3wlGZM",
        "outputId": "0d0ec150-e06a-4061-b377-37feac20324d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['am', 'i', 'learning']\n",
            "['<unk>', '<unk>', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = 3\n",
        "\n",
        "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n",
        "\n",
        "for word, freq in word_counts.items():\n",
        "    if freq == f:\n",
        "        print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfUEb5p_nT14",
        "outputId": "9eb32628-c9be-4070-eaa3-81201c8aa9ca"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "because\n",
            "learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\n",
        "training_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n",
        "\n",
        "test_set = ['i', 'am', 'learning']\n",
        "test_set_unk = ['i', 'am', '<UNK>']\n",
        "\n",
        "M = len(test_set)\n",
        "probability = 1\n",
        "probability_unk = 1\n",
        "\n",
        "bigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\n",
        "bigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n",
        "\n",
        "for i in range(len(test_set) - 2 + 1):\n",
        "    bigram = tuple(test_set[i: i + 2])\n",
        "    probability = probability * bigram_probabilities[bigram]\n",
        "\n",
        "    bigram_unk = tuple(test_set_unk[i: i + 2])\n",
        "    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n",
        "\n",
        "perplexity = probability ** (-1 / M)\n",
        "perplexity_unk = probability_unk ** (-1 / M)\n",
        "\n",
        "print(f\"perplexity for the training set: {perplexity}\")\n",
        "print(f\"perplexity for the training set with <UNK>: {perplexity_unk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpxq5XR1nruy",
        "outputId": "dafa8206-cfd7-447c-9fae-a1d6f428e88c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity for the training set: 1.2599210498948732\n",
            "perplexity for the training set with <UNK>: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#smoothing\n",
        "def add_k_smoothing_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
        "    numerator = n_gram_count + k\n",
        "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
        "    return numerator / denominator\n",
        "\n",
        "trigram_probabilities = {('i', 'am', 'happy') : 2}\n",
        "bigram_probabilities = {( 'i', 'am') : 10}\n",
        "vocabulary_size = 5\n",
        "k = 1\n",
        "\n",
        "probability_known_trigram = add_k_smoothing_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')],\n",
        "                           bigram_probabilities[( 'i', 'am')])\n",
        "\n",
        "probability_unknown_trigram = add_k_smoothing_probability(k, vocabulary_size, 0, 0)\n",
        "\n",
        "print(f\"probability_known_trigram: {probability_known_trigram}\")\n",
        "print(f\"probability_unknown_trigram: {probability_unknown_trigram}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltND9Mfbrk2_",
        "outputId": "45b9823e-00f6-4327-99d5-2c5f8fc8431d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability_known_trigram: 0.2\n",
            "probability_unknown_trigram: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#backoff\n",
        "trigram_probabilities = {('i', 'am', 'happy'): 0}\n",
        "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
        "unigram_probabilities = {'happy': 0.4}\n",
        "trigram = ('are', 'you', 'happy')\n",
        "bigram = trigram[1: 3]\n",
        "unigram = trigram[2]\n",
        "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
        "lambda_factor = 0.4\n",
        "probability_hat_trigram = 0\n",
        "if trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n",
        "    print(f\"probability for trigram {trigram} not found\")\n",
        "\n",
        "    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n",
        "        print(f\"probability for bigram {bigram} not found\")\n",
        "\n",
        "        if unigram in unigram_probabilities:\n",
        "            print(f\"probability for unigram {unigram} found\\n\")\n",
        "            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n",
        "        else:\n",
        "            probability_hat_trigram = 0\n",
        "    else:\n",
        "        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\n",
        "else:\n",
        "    probability_hat_trigram = trigram_probabilities[trigram]\n",
        "\n",
        "print(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0QIgkiptToM",
        "outputId": "0fd5670e-f2b5-4a68-eaf5-328d1740b713"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "besides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n",
            "\n",
            "probability for trigram ('are', 'you', 'happy') not found\n",
            "probability for bigram ('you', 'happy') not found\n",
            "probability for unigram happy found\n",
            "\n",
            "probability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#interpolation\n",
        "trigram_probabilities = {('i', 'am', 'happy'): 0.15}\n",
        "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
        "unigram_probabilities = {'happy': 0.4}\n",
        "lambda_1 = 0.8\n",
        "lambda_2 = 0.15\n",
        "lambda_3 = 0.05\n",
        "trigram = ('i', 'am', 'happy')\n",
        "bigram = trigram[1: 3]\n",
        "unigram = trigram[2]\n",
        "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
        "probability_hat_trigram = lambda_1 * trigram_probabilities[trigram]\n",
        "+ lambda_2 * bigram_probabilities[bigram]\n",
        "+ lambda_3 * unigram_probabilities[unigram]\n",
        "\n",
        "print(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L36jo2WyAzy",
        "outputId": "f009783e-a767-4b0c-a09a-670c94466ddc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "besides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n",
            "\n",
            "estimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.data.path.append('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL16VU6x8OkL",
        "outputId": "4ab94e98-1db3-492d-fac7-35fef05c53d5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('en_US.twitter.txt','r') as f:\n",
        "  data=f.read()\n",
        "print(\"Data type:\", type(data))\n",
        "print(\"Number of letters:\", len(data))\n",
        "print(\"First 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[0:300])\n",
        "print(\"-------\")\n",
        "\n",
        "print(\"Last 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[-300:])\n",
        "print(\"-------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "yf-gPkQr92Vh",
        "outputId": "573dc541-3d45-49c8-955b-b2fbd8b8329d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type: <class 'str'>\n",
            "Number of letters: 3335477\n",
            "First 300 letters of the data\n",
            "-------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n",
            "Last 300 letters of the data\n",
            "-------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentences(data):\n",
        "  sentences=data.split('\\n')\n",
        "  sentences=[s.strip() for s in sentences]\n",
        "  sentences=[s for s in sentences if len(s)>0]\n",
        "\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "D4XDVYIA_Rnu"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"\"\"\n",
        "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
        "\"\"\"\n",
        "print(x)\n",
        "\n",
        "split_sentences(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HOsJEp9AevZ",
        "outputId": "ff3147b3-4855-4e14-da2e-e6868bec687a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I have a pen.\n",
            "I have an apple. \n",
            "Ah\n",
            "Apple pen.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(data):\n",
        "  tokenized_sentences=[]\n",
        "  for s in data:\n",
        "    s=s.lower()\n",
        "    s=nltk.word_tokenize(s)\n",
        "    tokenized_sentences.append(s)\n",
        "  return tokenized_sentences"
      ],
      "metadata": {
        "id": "qmjoNcC1AjLo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen=split_sentences(x)\n",
        "t_s=tokenize_sentences(sen)\n",
        "print(t_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhTkMD-RCZaQ",
        "outputId": "47e19658-0721-4fe3-8d22-2356b99403cf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'have', 'a', 'pen', '.'], ['i', 'have', 'an', 'apple', '.'], ['ah'], ['apple', 'pen', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
        "tokenize_sentences(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-D1K8GUCor4",
        "outputId": "2cfbc8fb-bb49-4e18-c037-703cf6d4fb46"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green', '.'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_data(data):\n",
        "  sentences=split_sentences(data)\n",
        "  tokenized_sentences=tokenize_sentences(sentences)\n",
        "\n",
        "  return tokenized_sentences"
      ],
      "metadata": {
        "id": "b8yMQanrC43O"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
        "tokenized_data(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHWaB2HDEJij",
        "outputId": "963f406a-e62c-4293-8b1f-1c3ecf32a466"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = tokenized_data(data)\n",
        "random.seed(87)\n",
        "random.shuffle(tokenized_data)\n",
        "\n",
        "train_size = int(len(tokenized_data) * 0.8)\n",
        "train_data = tokenized_data[0:train_size]\n",
        "test_data = tokenized_data[train_size:]"
      ],
      "metadata": {
        "id": "SRICB9BHERgx"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{} data are split into {} train and {} test set\".format(\n",
        "    len(tokenized_data), len(train_data), len(test_data)))\n",
        "\n",
        "print(\"First training sample:\")\n",
        "print(train_data[0])\n",
        "\n",
        "print(\"First test sample\")\n",
        "print(test_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHHRa28TE11T",
        "outputId": "52d9d260-c846-4424-d053-d94907f72e89"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47961 data are split into 38368 train and 9593 test set\n",
            "First training sample:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
            "First test sample\n",
            "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(tokenized_sentences):\n",
        "  word_count={}\n",
        "  for sentence in tokenized_sentences:\n",
        "    for s in sentence:\n",
        "      if s not in word_count.keys():\n",
        "        word_count[s]=1\n",
        "      else:\n",
        "        word_count[s]+=1\n",
        "\n",
        "  return word_count"
      ],
      "metadata": {
        "id": "DWl4U0mgFBRM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "count_words(tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-PBKcElGaB0",
        "outputId": "d8306b40-6dd4-4092-d4c5-08e40588c18c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sky': 1,\n",
              " 'is': 1,\n",
              " 'blue': 1,\n",
              " '.': 3,\n",
              " 'leaves': 1,\n",
              " 'are': 2,\n",
              " 'green': 1,\n",
              " 'roses': 1,\n",
              " 'red': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_with_nplus_frequency(tokenized_sentences,count_threshold):\n",
        "  closed_vocab=[]\n",
        "  words_freq=count_words(tokenized_sentences)\n",
        "  for word,f in words_freq.items():\n",
        "    if f>=count_threshold:\n",
        "      closed_vocab.append(word)\n",
        "\n",
        "  return closed_vocab"
      ],
      "metadata": {
        "id": "FmfDyAKeGz1C"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
        "print(f\"Closed vocabulary:\")\n",
        "print(tmp_closed_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKl8aiNTIGVX",
        "outputId": "fc045a66-6828-438c-de87-911b2e74f91a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closed vocabulary:\n",
            "['.', 'are']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_words(tokenized_sentences,vocabulary,unknown_token='<unk>'):\n",
        "  vocabulary=set(vocabulary)\n",
        "  replaced_tokenized_sentences=[]\n",
        "  for sentence in tokenized_sentences:\n",
        "    replaced_sentence=[]\n",
        "    for s in sentence:\n",
        "      if s not in vocabulary:\n",
        "        replaced_sentence.append(unknown_token)\n",
        "      else:\n",
        "        replaced_sentence.append(s)\n",
        "\n",
        "    replaced_tokenized_sentences.append(replaced_sentence)\n",
        "\n",
        "  return replaced_tokenized_sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "95qxHKr5IUv1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
        "vocabulary = [\"dogs\", \"sleep\"]\n",
        "tmp_replaced_tokenized_sentences = replace_oov_words(tokenized_sentences, vocabulary)\n",
        "print(f\"Original sentence:\")\n",
        "print(tokenized_sentences)\n",
        "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
        "print(tmp_replaced_tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCG7zxDDVLdF",
        "outputId": "f3da4086-a57c-4e65-9553-eba7ce98dae4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence:\n",
            "[['dogs', 'run'], ['cats', 'sleep']]\n",
            "tokenized_sentences with less frequent words converted to '<unk>':\n",
            "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(train_data,test_data,count_threshold,unknown_token='<unk>'):\n",
        "  vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)\n",
        "  train_data_replaced = replace_oov_words(train_data,vocabulary,unknown_token=unknown_token)\n",
        "  test_data_replaced = replace_oov_words(test_data,vocabulary,unknown_token=unknown_token)\n",
        "\n",
        "  return train_data_replaced, test_data_replaced, vocabulary"
      ],
      "metadata": {
        "id": "jhX_VX7UVlZo"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_train = [['sky', 'is', 'blue', '.'],\n",
        "     ['leaves', 'are', 'green']]\n",
        "tmp_test = [['roses', 'are', 'red', '.']]\n",
        "\n",
        "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train,\n",
        "                                                           tmp_test,\n",
        "                                                           count_threshold = 1\n",
        "                                                          )\n",
        "\n",
        "print(\"tmp_train_repl\")\n",
        "print(tmp_train_repl)\n",
        "print()\n",
        "print(\"tmp_test_repl\")\n",
        "print(tmp_test_repl)\n",
        "print()\n",
        "print(\"tmp_vocab\")\n",
        "print(tmp_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J9jOd-eZ91x",
        "outputId": "6f423635-6641-4d41-8580-5c6dbb0546c3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tmp_train_repl\n",
            "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
            "\n",
            "tmp_test_repl\n",
            "[['<unk>', 'are', '<unk>', '.']]\n",
            "\n",
            "tmp_vocab\n",
            "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minimum_freq = 2\n",
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data,\n",
        "                                                                        test_data,\n",
        "                                                                        minimum_freq)"
      ],
      "metadata": {
        "id": "OoTMdYUybE47"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First preprocessed training sample:\")\n",
        "print(train_data_processed[0])\n",
        "print()\n",
        "print(\"First preprocessed test sample:\")\n",
        "print(test_data_processed[0])\n",
        "print()\n",
        "print(\"First 10 vocabulary:\")\n",
        "print(vocabulary[0:10])\n",
        "print()\n",
        "print(\"Size of vocabulary:\", len(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdfhZ4eobfN_",
        "outputId": "2558b179-c806-4eab-8e0c-123d5646073e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First preprocessed training sample:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
            "\n",
            "First preprocessed test sample:\n",
            "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']\n",
            "\n",
            "First 10 vocabulary:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']\n",
            "\n",
            "Size of vocabulary: 14823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_ngrams(data,n,start_token='<s>',end_token='<e>'):\n",
        "  n_grams={}\n",
        "  for sentence in data:\n",
        "    sentence=[start_token]*n+sentence+[end_token]\n",
        "    sentence=tuple(sentence)\n",
        "\n",
        "    for i in range(len(sentence) if n==1 else len(sentence)-n+1):\n",
        "      n_gram=sentence[i:i+n]\n",
        "      if n_gram in n_grams.keys():\n",
        "        n_grams[n_gram]+=1\n",
        "      else:\n",
        "        n_grams[n_gram]=1\n",
        "\n",
        "  return n_grams\n",
        "\n"
      ],
      "metadata": {
        "id": "mCExDVhfbj5p"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "print(\"Uni-gram:\")\n",
        "print(count_ngrams(sentences, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_ngrams(sentences, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzl2w0SzeYpR",
        "outputId": "66b51415-b099-4e7e-d054-049695b8a85a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uni-gram:\n",
            "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
            "Bi-gram:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probability(word,previous_ngram,n_gram_counts,nplus1_gram_counts,vocab_size,k=1.0):\n",
        "  previous_ngram=tuple(previous_ngram)\n",
        "  previous_ngram_count=n_gram_counts[previous_ngram] if previous_ngram in n_gram_counts else 0\n",
        "  denominator=previous_ngram_count+k*vocab_size\n",
        "  n_plus1_gram=previous_ngram+(word,)\n",
        "  n_plus1_gram_count=nplus1_gram_counts[n_plus1_gram] if n_plus1_gram in nplus1_gram_counts else 0\n",
        "  numerator=n_plus1_gram_count+k\n",
        "\n",
        "  probability=numerator/denominator\n",
        "  return probability\n"
      ],
      "metadata": {
        "id": "pboKIfZGec46"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_ngrams(sentences, 1)\n",
        "bigram_counts = count_ngrams(sentences, 2)\n",
        "tmp_prob = estimate_probability(\"cat\", [\"a\"], unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYkiGGTXiZH5",
        "outputId": "16f30903-a351-4dea-caf9-6cd95e94becd"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probabilities(previous_ngram,n_gram_counts,n_plus1_gram_counts,vocabulary,end_token='<e>',unk_token='<unk>',k=1.0):\n",
        "  previous_ngram=tuple(previous_ngram)\n",
        "  vocabulary=vocabulary+[end_token,unk_token]\n",
        "  vocabulary_size=len(vocabulary)\n",
        "  probabilities={}\n",
        "  for word in vocabulary:\n",
        "    probability=estimate_probability(word,previous_ngram,n_gram_counts,n_plus1_gram_counts,vocabulary_size,k=k)\n",
        "    probabilities[word]=probability\n",
        "\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "NI_ItX_Dip5b"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "unigram_counts = count_ngrams(sentences, 1)\n",
        "bigram_counts = count_ngrams(sentences, 2)\n",
        "\n",
        "estimate_probabilities([\"a\"], unigram_counts, bigram_counts, unique_words, k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zmilA880rV8",
        "outputId": "8b0f271a-9b83-4f68-8a22-c0db8fa98572"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dog': 0.09090909090909091,\n",
              " 'i': 0.09090909090909091,\n",
              " 'this': 0.09090909090909091,\n",
              " 'a': 0.09090909090909091,\n",
              " 'is': 0.09090909090909091,\n",
              " 'cat': 0.2727272727272727,\n",
              " 'like': 0.09090909090909091,\n",
              " '<e>': 0.09090909090909091,\n",
              " '<unk>': 0.09090909090909091}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_counts = count_ngrams(sentences, 3)\n",
        "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2HbYk_P0vMt",
        "outputId": "3ec45de4-cdba-4d48-f4b8-37be1858f1a0"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dog': 0.09090909090909091,\n",
              " 'i': 0.18181818181818182,\n",
              " 'this': 0.18181818181818182,\n",
              " 'a': 0.09090909090909091,\n",
              " 'is': 0.09090909090909091,\n",
              " 'cat': 0.09090909090909091,\n",
              " 'like': 0.09090909090909091,\n",
              " '<e>': 0.09090909090909091,\n",
              " '<unk>': 0.09090909090909091}"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCX7LQFW1DAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}